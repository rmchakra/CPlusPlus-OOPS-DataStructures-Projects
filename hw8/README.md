# CS 104 Student Repository

- **Name**: Rohan Chakraborty
- **USC ID**: 1233719421
- **Email**: rmchakra@usc.edu

Which size of input cases did you use?
1000000
100000
10000
1000
100
 Or in terms of exact values
1016429
107128
10202
1074
100

For each of the three collision resolution approaches (linear probing, quadratic probing, double hashing), report how long each of your input cases took. How long did it take per operation?

Linear probing:			
Number of Words Added	Time/iteration	iterations	Exact number of words
1000000					0.38				1		1016429
100000					0.043				20		107128
10000					0.0053				30		10202
1000					0.00076				40		1074
100						0.0000099			50		100
			
Quadratic:			
Number of Words Added	Time/iteration	iterations	Exact number of words
1000000					0.37				1		1016429
100000					0.044				20		107128
10000					0.0055				30		10202
1000					0.0009				40		1074
100						0.0001				50		100
			
Double Hashing			
Number of Words Added	Time/iteration	iterations	Exact number of words
1000000					0.57			10			1016429
100000					0.068			20			107128
10000					0.0081			30			10202
1000					0.0013			40			1074
100						0.000137		50			100


Try to plot the time per operation for each of the three approaches (ideally in one plot). Gnuplot is a free program doing this pretty easily, and as students, you also have free access to Matlab and Mathematica. Excel would be another option.
This is in the excel file Hashing_Plot


Explain why you think the results turned out the way they did.
Quadratic was faster than linear as it has reduced clumping. Linear probing simply inserts consequetively. so existing clusters have an increased probability of growing larger. If you inserted at i, then i+1, then i+2, each of those would lead to a collision and if there is a collision at i, it needs to go to i+1, then i+2, then i+3. i+1 similarly would need to go to i+2 then i+3. Increasing the cluster size even larger. So if theres a higher probability of certain kinds of values appearing due to the input, these clusters would grow larger and larger.
Contrastingly, quadratic hashing inserts in the order i+1, i+4, i+9, . . . , i+j2,
the sequences starting from i and i−1 are very different looking, the values wont cluster as much. For instance, suppose that key k is eventually
stored at i + 9, because i + 1 and i + 4 were also already in use. Now, key k′ is being inserted, which belongs in h(k′) = i + 1. Position i + 1 is already taken, but k′ now tries positions i + 2, i + 5, i + 10, . . .,
so it won’t run through the unsuccessful sequence i + 4, i + 9. The only clustering we could get will be from actual collisions.

Double hashing in my particular case could have been slower simply due to a random bad case of input occuring. In general however, Double hashing should be faster than quadratic probing.
This is because the sequence of positions explored for a key k depends not only on the position i where it originally belonged, but also on the actual
key k itself. So different keys, even if colliding under h, will be probing different sequences due to the different values returned by the second hash function h'. If i = h(k) is taken, k next probes the positions i + h′(k), i + 2h′(k), i + 3h′(k), . . .. This way, we spread out the probe positions much better
across elements.




How do you think the running time would compare to each version of probing if you were to implement your Map in the following fashions? Briefly justify each, although you do not need to quantify how much slower/faster with any specificity.

Running time of each probing for the hash table is effectively (amortised)constant since it is based on the randomised position generated by the hash functions. After rehashing, where the load factor is lower, the number of collisions would be quite low, which means lower probing required to find a position, even though resizing the hash function would take O(n) time. Thus the runtime would be dependent on the load factor. Thus is faster than all of the below (except unsorted list) which have insertion times of

Unsorted List -  Constant insertion at the end of the list, and hence has faster insertion since no probing is required. Find would however be much slower as would require O(n) time.

Sorted list - slower insert as if inserting to the first position, would need to shift every element back by a position giving a runtime of O(n). Look up would also be slower as would be O(log n)
Binary search tree, non-rotating variety - worst case you insert in an increasing or decreasing order, which means for every look up and every insert, the runtime would be O(n).
Binary search tree with AVL balancing. Runtime of a rotating tree is always log(n).
